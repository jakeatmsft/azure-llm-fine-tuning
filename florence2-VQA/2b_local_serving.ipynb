{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796cf06d-e4d3-422e-8301-a961e9520f52",
   "metadata": {},
   "source": [
    "# Open Source LLM serving using the Azure ML Python SDK\n",
    "\n",
    "[Note] Please use `Python 3.10 - SDK v2 (azureml_py310_sdkv2)` conda environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb139b1d-500c-4ef2-9af3-728f2a5ea05f",
   "metadata": {},
   "source": [
    "## 1. Load config file\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ef0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5234c47-b3e5-4218-8a98-3988c8991643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open('config.yml') as f:\n",
    "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "AZURE_SUBSCRIPTION_ID = d['config']['AZURE_SUBSCRIPTION_ID']\n",
    "AZURE_RESOURCE_GROUP = d['config']['AZURE_RESOURCE_GROUP']\n",
    "AZURE_WORKSPACE = d['config']['AZURE_WORKSPACE']\n",
    "AZURE_DATA_NAME = d['config']['AZURE_DATA_NAME']    \n",
    "DATA_DIR = d['config']['DATA_DIR']\n",
    "CLOUD_DIR = d['config']['CLOUD_DIR']\n",
    "HF_MODEL_NAME_OR_PATH = d['config']['HF_MODEL_NAME_OR_PATH']\n",
    "IS_DEBUG = d['config']['IS_DEBUG']\n",
    "\n",
    "azure_env_name = d['serve']['azure_env_name']\n",
    "azure_model_name = d['serve']['azure_model_name']\n",
    "azure_endpoint_name = d['serve']['azure_endpoint_name']\n",
    "azure_deployment_name = d['serve']['azure_deployment_name']\n",
    "azure_serving_cluster_size = d['serve']['azure_serving_cluster_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9843e0f-3cf1-4e86-abb7-a49919fac8d4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Serving preparation\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. Configure workspace details\n",
    "\n",
    "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccb4a273-ba31-4f47-a2fd-dc8cdea390f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We could not find config.json in: . or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import time\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    ml_client = MLClient(credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e496e364-c8ed-43c1-a248-2530c1eb44a4",
   "metadata": {},
   "source": [
    "### 2.4. Serving script\n",
    "\n",
    "If you are not serving with MLflow but with a custom model, you are free to write your own code.The `score.py` example below shows how to write the code.\n",
    "\n",
    "-   `init()`: This function is the place to write logic for global initialization operations like loading the LLM model.\n",
    "-   `run()`: Inference logic is called for every invocation of the endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0335b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import base64\n",
    "import logging\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoProcessor, BitsAndBytesConfig, get_scheduler\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def run_example_base64(task_prompt, text_input, base64_image, params):\n",
    " \n",
    "    max_new_tokens = params[\"max_new_tokens\"]\n",
    "    num_beams = params[\"num_beams\"]\n",
    "    \n",
    "    image = Image.open(BytesIO(base64.b64decode(base64_image)))\n",
    "    prompt = task_prompt + text_input\n",
    "\n",
    "    # Ensure the image is in RGB mode\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=num_beams\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\n",
    "    return parsed_answer\n",
    "\n",
    "\n",
    "def init():\n",
    "    \"\"\"\n",
    "    This function is called when the container is initialized/started, typically after create/update of the deployment.\n",
    "    You can write the logic here to perform init operations like caching the model in memory\n",
    "    \"\"\"\n",
    "    global model\n",
    "    global processor\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # Please provide your model's folder name if there is one\n",
    "    model_name_or_path = os.path.join(\n",
    "        os.getenv(\"AZUREML_MODEL_DIR\"), \"outputs\"\n",
    "    )\n",
    "    \n",
    "    model_kwargs = dict(\n",
    "        trust_remote_code=True,\n",
    "        revision=\"refs/pr/6\",        \n",
    "        device_map=device\n",
    "    )\n",
    "    \n",
    "    processor_kwargs = dict(\n",
    "        trust_remote_code=True,\n",
    "        revision=\"refs/pr/6\"\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, **model_kwargs)\n",
    "    processor = AutoProcessor.from_pretrained(model_name_or_path, **processor_kwargs)    \n",
    "\n",
    "    logging.info(\"Loaded model.\")\n",
    "    \n",
    "def run(json_data: str):\n",
    "    logging.info(\"Request received\")\n",
    "    data = json.loads(json_data)\n",
    "    task_prompt = data[\"task_prompt\"]\n",
    "    text_input = data[\"text_input\"]\n",
    "    base64_image = data[\"image_input\"]\n",
    "    params = data['params']\n",
    "\n",
    "    generated_text = run_example_base64(task_prompt, text_input, base64_image, params)\n",
    "    json_result = {\"result\": str(generated_text)}\n",
    "    \n",
    "    return json_result    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12c5cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    \"\"\"\n",
    "    This function is called when the container is initialized/started, typically after create/update of the deployment.\n",
    "    You can write the logic here to perform init operations like caching the model in memory\n",
    "    \"\"\"\n",
    "    global model\n",
    "    global processor\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # Please provide your model's folder name if there is one\n",
    "    #C:\\repo\\azure-llm-fine-tuning\\florence2-VQA\\artifact_downloads\\florence2-vqa-finetune\\outputs\\tokenizer.json\n",
    "    model_name_or_path = os.path.join('./artifact_downloads/florence2-vqa-finetune' , \"outputs\")       \n",
    "    model_kwargs = dict(\n",
    "        trust_remote_code=True,\n",
    "        revision=\"refs/pr/6\",        \n",
    "        device_map=device\n",
    "    )\n",
    "    \n",
    "    processor_kwargs = dict(\n",
    "        trust_remote_code=True,\n",
    "        revision=\"refs/pr/6\"\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, **model_kwargs)\n",
    "    processor = AutoProcessor.from_pretrained(model_name_or_path, **processor_kwargs)    \n",
    "\n",
    "    logging.info(\"Loaded model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cf9542c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\vft\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "c:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\vft\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jacwang\\.cache\\huggingface\\hub\\models--microsoft--Florence-2-base-ft. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Florence-2-base-ft:\n",
      "- processing_florence2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f437f6-153a-42d5-ab22-0011d0fe2481",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Test\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1. Inference\n",
    "\n",
    "Try calling the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "adf37c63-4d1e-44a3-8c94-268ffc716da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': \"{'DocVQA': 'Extraction'}\"}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "\n",
    "\n",
    "with open('./test_images/DocumentVQA_Test_01.jpg', 'rb') as img:\n",
    "    base64_img = base64.b64encode(img.read()).decode('utf-8')\n",
    "    \n",
    "sample = {\n",
    "    \"task_prompt\": \"DocVQA\",\n",
    "    \"image_input\": base64_img,\n",
    "    \"text_input\": \"Extract the following fields is the Name, Location, Extension?\", \n",
    "    \"params\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"num_beams\": 3\n",
    "    }\n",
    "}\n",
    "run(json.dumps(sample))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
